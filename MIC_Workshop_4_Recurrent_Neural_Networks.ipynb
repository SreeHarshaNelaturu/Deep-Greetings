{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIC Workshop 4 : Recurrent Neural Networks_with answers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SreeHarshaNelaturu/Deep-Greetings/blob/master/MIC_Workshop_4_Recurrent_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qml7Iv9sKwYG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MIC Workshop 4: Recurrent Neural Networks\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RNrL3aTpywZr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Background\n",
        "Hope you're enjoying the first Deep Learning workshop so far! For these workshops, we'll typically use Google Colab, an online coding environment. This is so that we don't have to worry about installing all of the libraries on everyone's different computers. \n",
        "\n",
        "_____\n",
        "You're now working in a Notebook. Notebooks have **cells**, each of which can be run by hitting Shift+Enter. Try it on the cell below!\n",
        "\n",
        "_You will see the output of the particular cell right below it_"
      ]
    },
    {
      "metadata": {
        "id": "LIU7z14RW_Nu",
        "colab_type": "code",
        "outputId": "7c03c7ef-a08f-4caf-e2da-6c073bcdc27c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Notebooks are so much fun!!!\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Notebooks are so much fun!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BfUb8YzkFQRz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In general, notebooks are a very suitable tool for machine learning/data science. We would also recommend trying [Jupyter Notebook](https://jupyter.org/install) if you haven't already\n",
        "\n",
        "_For later workshops, we might opt out for a more involved environment like Docker in case we want to do anything fancier_\n",
        "____\n",
        "## Installing PyTorch\n",
        "Don't worry too much about the contents of this cell. It basically just installs the right packages for you to run PyTorch code\n",
        "\n",
        "If this cell is causing problems for you (like `tcmalloc`,  make sure you click \"connect to Hosted runtime\" from the dropdown menu in the top right)\n"
      ]
    },
    {
      "metadata": {
        "id": "BsvMU7Q7KwzP",
        "colab_type": "code",
        "outputId": "8aa601b3-7a08-46cf-f654-ef7c72b255c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "# Installing pytorch, don't worry about the code in this cell. \n",
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = 'cpu' #cuda_output[0] if exists('/dev/nvidia0')\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "!pip install unidecode\n",
        "!pip install tensorboardX\n",
        "\n",
        "#!pip install tqdm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\r\u001b[K    4% |█▍                              | 10kB 19.1MB/s eta 0:00:01\r\u001b[K    8% |██▉                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K    13% |████▏                           | 30kB 2.5MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 40kB 1.7MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 51kB 2.1MB/s eta 0:00:01\r\u001b[K    26% |████████▍                       | 61kB 2.5MB/s eta 0:00:01\r\u001b[K    30% |█████████▊                      | 71kB 2.8MB/s eta 0:00:01\r\u001b[K    34% |███████████▏                    | 81kB 3.2MB/s eta 0:00:01\r\u001b[K    39% |████████████▌                   | 92kB 2.5MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 102kB 2.7MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 112kB 2.7MB/s eta 0:00:01\r\u001b[K    52% |████████████████▊               | 122kB 3.9MB/s eta 0:00:01\r\u001b[K    56% |██████████████████              | 133kB 3.9MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▌            | 143kB 7.2MB/s eta 0:00:01\r\u001b[K    65% |████████████████████▉           | 153kB 7.2MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 163kB 7.2MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 174kB 7.1MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████       | 184kB 7.2MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 194kB 36.9MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▉    | 204kB 8.2MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▎  | 215kB 8.4MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▋ | 225kB 8.4MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 235kB 7.4MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.0.22\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/d2/e08fe62f3554fbba081e80f6b23128df53b2f74ed4dcde73ec4a84dc53fb/tensorboardX-1.4-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.14.6)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (40.5.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BsD1YE9g76O2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Imports\n",
        "\n",
        "The imports for the following exercise are the following :\n",
        "\n",
        "**torch.nn** :  Allows for creation of neural network class\n",
        "\n",
        "**torch.autograd** : torch.autograd provides classes and functions implementing automatic differentiation\n",
        "\n",
        "**tensorboardX** : PyTorch package for Tensorboard visualization"
      ]
    },
    {
      "metadata": {
        "id": "zOGCa5JDK17n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from IPython import display\n",
        "import tqdm\n",
        "import time\n",
        "from tensorboardX import SummaryWriter\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r4kame7P215c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "use_cuda = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WvU9zelr7ivL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#(Optional Exercise)\n",
        "\n",
        "###Use PyTorch to implement cell description."
      ]
    },
    {
      "metadata": {
        "id": "6x4ITg_Fv7zY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Elman Cell Computation\n",
        "An Elman RNN cell with tanh or ReLU non-linearity.\n",
        "\n",
        "$h' = \\tanh(w_{ih} x + b_{ih}  +  w_{hh} h + b_{hh})$"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "L065AM7VUfSa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "9049551d-14b2-4317-9f97-358327dd600d"
      },
      "cell_type": "code",
      "source": [
        "class RNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        #Implement initializations\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        #Implement forward pass\n",
        "        return output, hidden"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-830d951a4549>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    def forward(self, input, hidden):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2oQt7o-GtzT7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### LSTM Cell Computation\n",
        "Implement LSTM cell computation described by the following expression"
      ]
    },
    {
      "metadata": {
        "id": "cN4pgOp-syDx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
        "f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
        "g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
        "o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
        "c_t = f_t c_{(t-1)} + i_t g_t \\\\\n",
        "h_t = o_t \\tanh(c_t)$\n"
      ]
    },
    {
      "metadata": {
        "id": "a-kRdJRYsKmM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        #Implement initializations\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        #Implement forward pass\n",
        "        return output, hidden, cell\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zK3wy8XmLM1d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "This dataset is a subset of works by William Shakespeare, this cell block downloads the data, reads and pre-processes and displays an example.\n",
        "\n",
        "\n",
        "You don't need to worry about the format too much now. But in general, getting the data in the necessary format is usually a key (albeit mundane) part of the process. "
      ]
    },
    {
      "metadata": {
        "id": "52KWKp--LRVH",
        "colab_type": "code",
        "outputId": "46c0d3c6-6b32-4049-96a1-4620c66d12e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import string\n",
        "import random\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "def DownloadFile(url):\n",
        "    local_filename = url.split('/')[-1]\n",
        "    r = requests.get(url)\n",
        "    return r.text\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "  \n",
        "target_url = \"https://raw.githubusercontent.com/cos495/code/master/shakespeare.txt\"\n",
        "data = DownloadFile(target_url)\n",
        "print(data[10:100])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "zen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dbXyGCo52dmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Pre-processing\n",
        "\n",
        "We take in the downloaded data and create training sets, which are encoded in the form of torch tensors for feeding into the neural network."
      ]
    },
    {
      "metadata": {
        "id": "5Jl_FxV3-ZxX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        try:\n",
        "            tensor[c] = all_characters.index(string[c])\n",
        "        except:\n",
        "            continue\n",
        "    return tensor  \n",
        "\n",
        "def random_training_set(chunk_len, batch_size, file):\n",
        "    inp = torch.LongTensor(batch_size, chunk_len)\n",
        "    target = torch.LongTensor(batch_size, chunk_len)\n",
        "    for bi in range(batch_size):\n",
        "        start_index = random.randint(0, len(file) - chunk_len)\n",
        "        end_index = start_index + chunk_len + 1\n",
        "        chunk = file[start_index:end_index]\n",
        "        inp[bi] = char_tensor(chunk[:-1])\n",
        "        target[bi] = char_tensor(chunk[1:])\n",
        "    inp = Variable(inp)\n",
        "    target = Variable(target)\n",
        "    return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uw7E1yvHPbMD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Model\n",
        "\n",
        "#RNN Cell Structure\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/759/1*UkI9za9zTR-HL8uM15Wmzw.png)\n",
        "\n",
        "\n",
        "The CharRNN (based on Andrej Karpathy's model described in [Unreasonable Effectiveness of RNNs by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)) works character by character and attempts to predict the next character. This is then assimilated to produce the output.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this program, we create the CharRNN class, in which we define the parameters of an encoder-decoder framework for text generation, you have the option of either using a vanilla RNN cell using nn.RNN or using a Long Short Term Memory unit (LSTM) using nn.LSTM.\n",
        "\n",
        "In this model, given a sequence of characters from this data (\"Shakespeare\") we train it to predict the next character in the sequence.\n",
        "\n",
        "You will be defining the parameters of the model, read through the documentation to get a feel of what the input to each line is is!\n",
        "\n",
        "\n",
        "\n",
        "[torch.nn.LSTM/RNN/Embedding](https://pytorch.org/docs/stable/nn.html)\n",
        "![alt text](https://chunml.github.io/ChunML.github.io/images/projects/sequence-to-sequence/repeated_vector.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Z9FA2RAisbzK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "First we will be taking in the data and create an encoding  by creating embeddings and this is will input to the RNN cell, which models the input and predicts the next character, this token is further decoded based on the embedding definition and displayed as the prediction.\n",
        "\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/00d11e704403d414e272c35bf89c9428e6d4d2ca/68747470733a2f2f692e696d6775722e636f6d2f4a4835387458592e706e67)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YM-RJy9gNcT3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://github.com/spro/char-rnn.pytorch\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, model, n_layers=1):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.model = model.lower()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        #https://pytorch.org/docs/stable/nn.html\n",
        "        \n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size) #nn.Embedding(??)   \n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers) #\"\"\"nn.RNN(?? )\"\"\"\n",
        "        if model==\"lstm\":\n",
        "          self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers) #\"\"\"nn.LSTM(??)\"\"\"\n",
        "          \n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        batch_size = input.size(0)\n",
        "        encoded = self.encoder(input) #\"\"\"self.encoder(??)\"\"\"\n",
        "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
        "        output = self.decoder(output.view(batch_size, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        if self.model == \"lstm\":\n",
        "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
        "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
        "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QjolCWdP1fkS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training\n",
        "\n",
        "Now that you have defined the forward pass of the neural network it is time to define the parameters for training.\n",
        "\n",
        "**Hidden Size** - This is the number of hidden units present in the RNN cell.\n",
        "\n",
        "**Learning Rate** - Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient.\n",
        "\n",
        "**Model** : RNN or LSTM (LSTM takes longer to train)\n",
        "\n",
        "**n_layers** - Number of RNN Layers, the recommended number is 1 or 2."
      ]
    },
    {
      "metadata": {
        "id": "KB9xsWiEQhkU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Iinitialize the model"
      ]
    },
    {
      "metadata": {
        "id": "ZDV_0GRbLtXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 100\n",
        "learning_rate = 0.01\n",
        "model = \"rnn\"\n",
        "n_layers = 2\n",
        "\n",
        "\"\"\"\n",
        "hidden_size = ??\n",
        "learning_rate = ??\n",
        "n_layers = ??\n",
        "model = ??\n",
        "\"\"\"\n",
        "decoder = CharRNN(\n",
        "    n_characters,\n",
        "    hidden_size,\n",
        "    n_characters,\n",
        "    model,\n",
        "    n_layers=n_layers,\n",
        ")\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if use_cuda:\n",
        "    decoder.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ilkYK2mE0HbQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**chunk_len** - Size of the random chunck of data used in every training pass\n",
        "\n",
        "**n_epochs** - Number of epochs for which the training will run.\n",
        "\n",
        "**print_every** - Frequency with which you want to print predictions\n",
        "\n",
        "**batch_size** - Number of training examples per batch (Number of chunks per batch)"
      ]
    },
    {
      "metadata": {
        "id": "GRdaN2-FRndV",
        "colab_type": "code",
        "outputId": "778765c9-e24e-4456-b6d9-412eb49f33e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#\"\"\"\n",
        "n_epochs = 1800\n",
        "chunk_len = 200\n",
        "print_every = 10\n",
        "batch_size = 100\n",
        "#\"\"\"\n",
        "\"\"\"\n",
        "n_epochs = ??\n",
        "chunk_len = ??\n",
        "print_every = ??\n",
        "batch_size = ??\n",
        "\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nn_epochs = ??\\nchunk_len = ??\\nprint_every = ??\\nbatch_size = ??\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "H3bFZTUxwBlm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Backpropagation"
      ]
    },
    {
      "metadata": {
        "id": "5KPmkvGJPkNt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden(batch_size)\n",
        "    if use_cuda:\n",
        "        hidden = hidden.cuda()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[:,c], hidden)\n",
        "        loss += criterion(output.view(batch_size, -1), target[:,c])\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / chunk_len, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V8M-FSO-TTLF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate Text\n",
        "\n",
        "In this cell, we are declaring a starting character which serves as the token or seed the first input, which the RNN will use to start predicting the next character\n",
        "\n",
        "**predict_len** - Length of prediction expected\n",
        "\n",
        "**Temperature** - Temperature is a hyperparameter of LSTMs (and neural networks generally) used to control the randomness of predictions by scaling the logits before applying softmax. \n",
        "\n",
        "![alt text](https://docs.google.com/uc?id=13nBWDE7RXlnMmT2g_BJQ4brFO7YIuh1Q)\n",
        "\n",
        "**cuda** - Run generate on GPU.\n"
      ]
    },
    {
      "metadata": {
        "id": "Gh4cIaRDS23s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8, cuda=False):\n",
        "    hidden = decoder.init_hidden(1)\n",
        "    prime_input = Variable(char_tensor(prime_str).unsqueeze(0))\n",
        "    \n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[:,p], hidden)\n",
        "        \n",
        "    inp = prime_input[:,-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = Variable(char_tensor(predicted_char).unsqueeze(0))\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t51_oWELy-CP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Optional - TensorBoardX\n",
        "\n",
        "In case you want to monitor your loss using TensorBoardX,  the following piece of code generates a local tunnel, follow the link to get the necessary scalar plot.\n",
        "\n",
        "##Click on the URL once you run the training op."
      ]
    },
    {
      "metadata": {
        "id": "QsgEtR_GkL1f",
        "colab_type": "code",
        "outputId": "000ed5c6-417a-4d32-826c-9cde1c30f68e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "#tensorboardX --host 0.0.0.0\n",
        "! npm install -g localtunnel\n",
        "writer = SummaryWriter('/tmp/log')\n",
        "get_ipython().system_raw('lt --port 6006 >> url.txt 2>&1 &')\n",
        "! cat url.txt\n",
        "\"\"\"\n",
        "\n",
        "! curl http://localhost:6006\n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1\n",
        "! unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "    \"\"\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/client\n",
            "+ localtunnel@1.9.1\n",
            "added 54 packages from 31 contributors in 2.399s\n",
            "your url is: https://dangerous-cow-67.localtunnel.me\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n! curl http://localhost:6006\\n! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1\\n! unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1\\n\\n! curl -s http://localhost:4040/api/tunnels | python3 -c     \"import sys, json; print(json.load(sys.stdin)[\\'tunnels\\'][0][\\'public_url\\'])\"\\n    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "riXDDbny1-DF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "loss_avg = 0\n",
        "writer = SummaryWriter()\n",
        "print(\"Training for %d epochs...\" % n_epochs)\n",
        "for epoch in tqdm.tqdm(range(1, n_epochs + 1)):\n",
        "    loss = train(*random_training_set(chunk_len, batch_size, data))[0]\n",
        "    loss_avg += loss\n",
        "    writer.add_scalar('Training Loss', loss, epoch)\n",
        "    #writer.add_histogram(\"hist\", (train(*random_training_set(chunk_len, batch_size, data))[1]).detach().numpy(), epoch)\n",
        "    if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "      print('loss: ', loss)\n",
        "      print(generate(decoder, 'Wh', 100, cuda=use_cuda), '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CmkwHEew0oU8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(loss_avg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7snNirbPzaDk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Let's try sampling with high temperature:"
      ]
    },
    {
      "metadata": {
        "id": "0oCPS6kzRK4w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate(decoder, prime_str=\"A\", temperature= 100, cuda=use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q1g9IOY-zmZ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Let's try sampling with low temperature:"
      ]
    },
    {
      "metadata": {
        "id": "77ijhzn9Tc1C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate(decoder, prime_str=\"A\", temperature= 0.001, cuda=use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O2KRkbkm1MT_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Feedback Survey\n",
        "Don't forget to fill out the feedback survey [here](https://goo.gl/rXV5EQ) for suggestions on how we can improve our future workshops!"
      ]
    },
    {
      "metadata": {
        "id": "I88wTPCJ1bls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Citations\n",
        "\n",
        "Adapted from https://github.com/spro/char-rnn.pytorch\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "L_BretBX238_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}